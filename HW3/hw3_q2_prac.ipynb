{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimsh96453/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hw3_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Compose, ToTensor, Normalize\n\u001b[1;32m      9\u001b[0m \u001b[39m#from MyMLP import MyMLP\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhw3_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_MNIST\n\u001b[1;32m     13\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m2023\u001b[39m)\n\u001b[1;32m     15\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hw3_utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "#from MyMLP import MyMLP\n",
    "\n",
    "from hw3_utils import load_MNIST\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "normalize_vals = (0.1307, 0.3081)\n",
    "\n",
    "# load MNIST dataset\n",
    "train_dataset, test_dataset, train_loader, test_loader = load_MNIST(batch_size, normalize_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, kernel_size, stride_size, max_pool_size, learning_rate, max_epochs):\n",
    "        '''\n",
    "        input_size: [int], feature dimension \n",
    "        hidden_size: number of hidden nodes in the hidden layer\n",
    "        output_size: number of classes in the dataset, \n",
    "        learning_rate: learning rate for gradient descent,\n",
    "        max_epochs: maximum number of epochs to run gradient descent\n",
    "        '''\n",
    "        ### Construct your MLP Here (consider the recommmended functions in homework writeup)  \n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 20, kernel_size = kernel_size, padding = 0, bias = True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = max_pool_size, stride = stride_size)\n",
    "        self.drop = nn.Dropout(p = 0.5)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(3380, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Function to do the forward pass with images x '''\n",
    "        ### Use the layers you constructed in __init__ and pass x through the network\n",
    "        ### and return the output\n",
    "        relu = nn.ReLU()\n",
    "        x = self.conv1(x)\n",
    "        x = relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_loader, criterion, optimizer):\n",
    "        '''\n",
    "        Function used to train the MLP\n",
    "\n",
    "        train_loader: includes the feature matrix and class labels corresponding to the training set,\n",
    "        criterion: the loss function used,\n",
    "        optimizer: which optimization method to train the model.\n",
    "        '''\n",
    "        total, err = 0, 0\n",
    "        prediction = []\n",
    "        # Epoch loop\n",
    "        for i in range(self.max_epochs):\n",
    "\n",
    "            # Mini batch loop\n",
    "            for j,(images,labels) in enumerate(train_loader, 0):\n",
    "                images, labels = (images, labels)\n",
    "                #images = images.view(-1, 28*28)\n",
    "                # Forward pass (consider the recommmended functions in homework writeup)\n",
    "                outputs = self.forward(images)\n",
    "\n",
    "                # Backward pass and optimize (consider the recommmended functions in homework writeup)\n",
    "                # Make sure to zero out the gradients using optimizer.zero_grad() in each loop\n",
    "                optimizer.zero_grad()\n",
    "                # Track the loss and error rate\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                prediction.append(pred)\n",
    "                total += labels.size(0)\n",
    "                err += (pred != labels).sum().item()\n",
    "            # Print/return training loss and error rate in each epoch\n",
    "            print({\"loss\": round(loss.item(), 4), \"err_rate\": round(err/total, 4)})\n",
    "\n",
    "    def predict(self, test_loader, criterion):\n",
    "        '''\n",
    "        Function used to predict with the MLP\n",
    "\n",
    "        test_loader: includes the feature matrix and classlabels corresponding to the test set,\n",
    "        criterion: the loss function used.\n",
    "        '''\n",
    "        total, err = 0, 0\n",
    "        missed_img = []\n",
    "        with torch.no_grad(): # no backprop step so turn off gradients\n",
    "            for j,(images,labels) in enumerate(test_loader, 0):\n",
    "                images, labels = (images, labels)\n",
    "                #images = images.view(-1, 28*28)\n",
    "                #labels_np = labels.numpy()\n",
    "                #label_full = np.hstack((label_full, labels_np))\n",
    "                # Compute prediction output and loss\n",
    "                outputs = self.forward(images)\n",
    "                # Measure loss and error rate and record\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, pred = torch.max(outputs.data, 1)\n",
    "                #pred_np = pred.numpy()\n",
    "                #prediction = np.hstack((prediction, pred_np))\n",
    "                total += labels.size(0)\n",
    "                err += (pred != labels).sum().item()\n",
    "                missed_img_elem = torch.squeeze(images[pred != labels])\n",
    "                missed_img.append(missed_img_elem.item())\n",
    "        # Print/return test loss and error rate\n",
    "        print({\"loss\": round(loss.item(), 4), \"err_rate\": round(err/total, 4)})\n",
    "        #label_full = np.delete(label_full, slice(0, 32))\n",
    "        #prediction = np.delete(prediction, slice(0, 32))\n",
    "\n",
    "        return pred, missed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "0.1\n",
      "{'loss': 0.156, 'err_rate': 0.0997}\n",
      "{'loss': 0.0279, 'err_rate': 0.0753}\n",
      "{'loss': 0.0642, 'err_rate': 0.0644}\n",
      "{'loss': 0.0527, 'err_rate': 0.0576}\n",
      "{'loss': 0.0144, 'err_rate': 0.0528}\n",
      "{'loss': 0.0102, 'err_rate': 0.0494}\n",
      "{'loss': 0.0015, 'err_rate': 0.0467}\n",
      "{'loss': 0.195, 'err_rate': 0.0447}\n",
      "{'loss': 0.1, 'err_rate': 0.043}\n",
      "{'loss': 0.0235, 'err_rate': 0.0415}\n",
      "test\n",
      "{'loss': 0.0026, 'err_rate': 0.0309}\n"
     ]
    }
   ],
   "source": [
    "lr = [0.1]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('SGD')\n",
    "for i in range(len(lr)):\n",
    "    print(lr[i])\n",
    "    cnn = MyCNN(input_size=28*28, output_size=10, kernel_size=3, stride_size=2, max_pool_size=2, learning_rate=lr[i], max_epochs=10)\n",
    "    optim_sgd = torch.optim.SGD(cnn.parameters(), lr = lr[i])\n",
    "    cnn_sgd = cnn.fit(train_loader = train_loader, criterion = criterion, optimizer = optim_sgd)\n",
    "    cnn_sgd\n",
    "    print(\"test\")\n",
    "    test_cnn_sgd, missed_img = cnn.predict(test_loader = test_loader, criterion = criterion)\n",
    "    test_cnn_sgd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(missed_img)):\n",
    "    missed_np = missed_img[i].numpy()\n",
    "    plt.imshow(missed_np, 'gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65e26a417bba122887fd29982395cb2c5358c401a000d55be7838ee8c3bbdd30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
